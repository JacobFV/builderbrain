# Production configuration for large-scale deployment
# GPT-Neo 2.7B or similar large model

model:
  type: "gpt_neo"
  name: "EleutherAI/gpt-neo-2.7B"
  hidden_size: 2560
  num_layers: 8
  num_programs: 32
  alpha_cap: 0.15

constraints:
  grammar:
    enabled: true
    target: 0.0
    normalizer: "rank"
  graph2graph:
    enabled: true
    target: 0.15
    normalizer: "rank"
  buildability:
    enabled: true
    target: 0.0
    normalizer: "winsor"
  reuse:
    enabled: true
    target: 0.3
    normalizer: "rank"
  calibration:
    enabled: true
    target: 0.05
    normalizer: "rank"

training:
  batch_size: 16
  learning_rate: 1e-4
  eta_lambda: 5e-3
  lambda_max: 50.0
  num_epochs: 100
  save_every: 20
  gradient_checkpointing: true
  mixed_precision: true

data:
  max_length: 1024
  vocab_size: 50257

runtime:
  max_generation_length: 200
  temperature: 0.7
  use_grammar_mask: true
  beam_search: true
  num_beams: 4
